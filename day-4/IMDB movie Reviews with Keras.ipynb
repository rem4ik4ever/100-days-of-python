{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing IMDB dataset moview reviews. Using Keras to determine is moview review was Positive(1) or Negative(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_data** \n",
    "Represents an array of numbers which essentially indexes of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary is not very helpful for us to decode review. Thus it will be nice to reverse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_indexes = dict([(val, key) for (key, val) in word_indexes.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick and easy function to decode review. Note we need to have offset of 3, (1. Padding, 2. Start of sequence, 3. Unknown) otherwise review will not make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(sequence, reversed_word_indexes):\n",
    "    review = ' '.join([reversed_word_indexes.get(i - 3, '?') for i in sequence])\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = decode_review(train_data[0], reversed_word_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an Example of decoded review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data\n",
    "\n",
    "Now that we know what those numbers represent, we can prepare our data to be passed to keras. We need to convers single dimension array into numpy matrix. Which is called One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_data(sequences, dimensions=10000):\n",
    "    matrix = np.zeros((len(sequences), dimensions))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "            matrix[i, sequence] = 1\n",
    "    return matrix\n",
    "x_train = vectorize_data(train_data)\n",
    "x_test = vectorize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For labels we can just convert it into numpy array of float32 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating our model\n",
    "\n",
    "Now that we have our data prepared, we can start working on our Keras model. \n",
    "We will have 2 Dense layers of 16 NN nodes with activation function 'relu' and last Dense layer will be our output layer of dense 1 with activation function 'sigmoid' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to compile our model and for that we need to provide 3 parameters:\n",
    " 1. optimizer function\n",
    " 2. loss function\n",
    " 3. metrics \n",
    "In our case optimizer 'rmsprop'\n",
    "loss_function will be 'binary_crossentropy' because we are dealing with binary classification where output can only be 0 or 1\n",
    "metrics will be 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to validate our approach and see if our model works. For that we will run validation process deviving train data into sets of 10000 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "x_partial_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "y_partial_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 15000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_val), len(x_partial_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/senpai/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 4s 252us/step - loss: 0.5095 - acc: 0.7901 - val_loss: 0.3739 - val_acc: 0.8740\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.3043 - acc: 0.8991 - val_loss: 0.3152 - val_acc: 0.8803\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.2253 - acc: 0.9235 - val_loss: 0.2851 - val_acc: 0.8866\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.1759 - acc: 0.9435 - val_loss: 0.2757 - val_acc: 0.8903\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.1443 - acc: 0.9535 - val_loss: 0.3239 - val_acc: 0.8743\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.1193 - acc: 0.9640 - val_loss: 0.2945 - val_acc: 0.8862\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.0980 - acc: 0.9715 - val_loss: 0.3442 - val_acc: 0.8755\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0804 - acc: 0.9778 - val_loss: 0.3937 - val_acc: 0.8682\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.0699 - acc: 0.9802 - val_loss: 0.3522 - val_acc: 0.8798\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0556 - acc: 0.9860 - val_loss: 0.3936 - val_acc: 0.8775\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 0.0454 - acc: 0.9903 - val_loss: 0.4483 - val_acc: 0.8642\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0378 - acc: 0.9920 - val_loss: 0.4303 - val_acc: 0.8753\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.4646 - val_acc: 0.8718\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0254 - acc: 0.9950 - val_loss: 0.5196 - val_acc: 0.8629\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0183 - acc: 0.9976 - val_loss: 0.5848 - val_acc: 0.8646\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 0.0125 - acc: 0.9990 - val_loss: 0.5633 - val_acc: 0.8660\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 0.0121 - acc: 0.9988 - val_loss: 0.5992 - val_acc: 0.8654\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 0.0092 - acc: 0.9989 - val_loss: 0.6391 - val_acc: 0.8648\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0090 - acc: 0.9985 - val_loss: 0.6625 - val_acc: 0.8637\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 0.0040 - acc: 0.9999 - val_loss: 0.8028 - val_acc: 0.8472\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_partial_train, y_partial_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done, let's now see if our model overfits and see how we can takle model to improve its result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "accuracy = history_dict['acc']\n",
    "validation_accuracy = history_dict['val_acc']\n",
    "\n",
    "loss = history_dict['loss']\n",
    "validation_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, 'bo', label='Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above our training accuracy keeps growing with every epoch, up to 99% however validation accuracy peaks on 4th epoch and then starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Loss')\n",
    "plt.plot(epochs, validation_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same applies to loss metrics. Loss going down to 0 with each epoch, however validation loss overfits after 4th epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally\n",
    "Let's train our model with final train and testing data for 4 epochs and see our final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 55us/step - loss: 0.4470 - acc: 0.8242\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.2557 - acc: 0.9103\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.1969 - acc: 0.9305\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.1645 - acc: 0.9423\n",
      "25000/25000 [==============================] - 1s 50us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3179986201286316, 0.87512]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a fairly naive approach, but even with this approach I managed to get 88% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29397136],\n",
       "       [0.9996812 ],\n",
       "       [0.9375764 ],\n",
       "       ...,\n",
       "       [0.16675383],\n",
       "       [0.15830699],\n",
       "       [0.7685168 ]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to change number of hidden units in input layers and Number of Layers to see what effect it has on model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 56us/step - loss: 0.4418 - acc: 0.8109\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.2470 - acc: 0.9110\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.1903 - acc: 0.9305\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.1569 - acc: 0.9430\n",
      "25000/25000 [==============================] - 1s 44us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31386076488494874, 0.87996]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 56us/step - loss: 0.4393 - acc: 0.8281\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.2686 - acc: 0.9097\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.2131 - acc: 0.9262\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.1824 - acc: 0.9369\n",
      "25000/25000 [==============================] - 1s 42us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2848528031826019, 0.88668]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22009897],\n",
       "       [0.99956304],\n",
       "       [0.8150622 ],\n",
       "       ...,\n",
       "       [0.13442782],\n",
       "       [0.06842265],\n",
       "       [0.45911583]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 58us/step - loss: 0.1688 - acc: 0.7778\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.0837 - acc: 0.9064\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.0615 - acc: 0.9286\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.0492 - acc: 0.9425\n",
      "25000/25000 [==============================] - 1s 44us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08502628820419311, 0.8856]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing loss function to 'mse' and decreasing number of layers in the model increased accuracy. Now lets try to combine them and see our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 56us/step - loss: 0.1405 - acc: 0.8253\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.0788 - acc: 0.9132\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.0613 - acc: 0.9306\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.0517 - acc: 0.9413\n",
      "25000/25000 [==============================] - 1s 43us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08568406512260437, 0.88636]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't work out as expected now lets try to increse number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 58us/step - loss: 0.1399 - acc: 0.8160: 0s - loss: 0.1485 - acc: 0.8\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.0726 - acc: 0.9111\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.0553 - acc: 0.9318\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.0445 - acc: 0.9459\n",
      "25000/25000 [==============================] - 1s 43us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08857040420293807, 0.88048]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not lets minimize number of hidden layers to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 0.1969 - acc: 0.7988\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.1309 - acc: 0.8807\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.0973 - acc: 0.9037\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.0782 - acc: 0.9177\n",
      "25000/25000 [==============================] - 1s 45us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09446858428239822, 0.88424]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to use 'tanh' activation function instead of 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 2s 61us/step - loss: 0.4106 - acc: 0.8282\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.2307 - acc: 0.9119\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.1779 - acc: 0.9342\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.1494 - acc: 0.9458\n",
      "25000/25000 [==============================] - 1s 47us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3290228146791458, 0.87608]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='tanh', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 2s 68us/step - loss: 0.1753 - acc: 0.7570\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.1051 - acc: 0.8764\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.0801 - acc: 0.9063\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.0641 - acc: 0.9256\n",
      "25000/25000 [==============================] - 1s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08422688279509544, 0.88552]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
